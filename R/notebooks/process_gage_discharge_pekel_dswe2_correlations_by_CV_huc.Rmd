---
title: "Processing stream gage info with Pekel and DSWE 2.0 data in California's Central Valley"
fontsize: 9
output: html_notebook
editor_options:
  chunk_output_type: inline
---



```{r, echo=FALSE, warning=FALSE, message=FALSE}

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## process_gage_discharge_pekel_dswe2_correlations_by_CV_huc.Rmd
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# ---------------------------------------------------------------------- #
# General setup ####
# ---------------------------------------------------------------------- #

# Load packages in a way that's easy for others to as well
# Credit due: Simon at https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them

loadPackages <- function(pkgs){
  for (pkg in pkgs){
    #  require returns TRUE invisibly if it was able to load package
    if(!require(pkg, character.only = TRUE)){
      #  If package was not able to load, re-install
      install.packages(pkg, dependencies = TRUE)
      #  Load package after installing
      require(pkg, character.only = TRUE)
    }
  }
}

# Load packages
loadPackages(c("dataRetrieval", "plyr", "tidyverse", 'data.table', 'knitr', 'kableExtra', 'devtools', 'googledrive'))

# Load mapping packages
loadPackages(c("ggmap", "sf", "maps", "mapdata", 'rgdal'))

# Set default as echo FALSE
knitr::opts_chunk$set(echo = FALSE)

```

```{r, echo=FALSE}

# ---------------------------------------------------------------------- #
#### Set no data threshold ####
# ---------------------------------------------------------------------- #

nodata_threshold <- 5

```


```{r, include = F, results = 'hide'}

# ---------------------------------------------------------------------- #
#### Set output files and folders ####
# ---------------------------------------------------------------------- #

path.out <-  "E:/projects/place/R"

file.out <- paste0("CV_dswe_pekel_data_", nodata_threshold, "_pct_nodata.Rmd")

# Set path to shapefile directory
shp.dir <- file.path(path.out, "shp")
ifelse(!dir.exists(shp.dir), dir.create(shp.dir), FALSE)


```
####Steps  

  - Pekel and DSWE monthly composites within all Central Valley HUCs retrieved from online Google Drive account
    - Both files processed to convert values, standardize names, etc. 
    - In scenes with > **`r nodata_threshold`%** No Data, the water values are zeroed out pending later interpolation
  - Metadata retrieved from NWIS for all gages in Central Valley
  - Gages assessed for length of data record, data type
    - Gage record has to include at least 5 years of data after 1984
    - Discharge or gage height
  - Streamgage data retrieved (NWIS)
  - Correlations calculated between gage data, Pelel, and DSWE in each HUC
  - Output file is written to **`r file.path(path.out, file.out)`**
  
  

```{r, echo  TRUE, message = FALSE, include= FALSE}

# ---------------------------------------------------------------------- #
#### Download HUC GIS data from Google Drive ####
# ---------------------------------------------------------------------- #

# HUCs
filename <- 'WBDHU8_Central_Valley_UTMz10.shp'
data_url = "1_2JnwZFS0blok23-9O7lw-o7Hk96E-nc" 
download.file(sprintf("https://docs.google.com/uc?id=%s&export=download", data_url), destfile = file.path(shp.dir, "huc.zip"), mode='wb')
unzip(zipfile = file.path(shp.dir, 'huc.zip'), exdir = shp.dir)
file.remove(file.path(shp.dir, 'huc.zip'))

shp <- st_read(dsn = file.path(shp.dir, filename), stringsAsFactors = F)
```


```{r, echo=TRUE}

# ---------------------------------------------------------------------- #
# ------ Function - Process Pekel data ---------
# ---------------------------------------------------------------------- #

process_pekel <- function(df, nodata_cap) {
  
  df$date <- as.Date(df$date, format = "%Y-%m-%d")
  
# First row is nonsense  
  df <- df[-1,]

# X0 = no data, # X1 = not water # X2 = water
  cols_to_keep <- c("date", "huc8", "X0", "X1", "X2")
  df <- df[, cols_to_keep]
  colnames(df) <- c("date", "huc8", "nodata", "notwater", "water")

# Convert from pixels in m^2 to km^2: 900 m^2/pixel x 1e-06 km^2/m^2
  df$water <- df$water * 0.0009  
  df$notwater <- df$notwater * 0.0009  
  df$nodata <- df$nodata * 0.0009  

# Convert "water" --> NA in rows where nodata exceeds <threshold>% of the total
  df$sum <- rowSums(df[, c("nodata", "notwater", "water")], na.rm = T)
  df$pct <- df$nodata/df$sum * 100
  df$water[df$pct > nodata_cap] <- NA

return(df)
}

```


```{r,echo=FALSE, message = FALSE, warning = FALSE}

# ---------------------------------------------------------------------- #
#### Get Pekel unbuffered data ####
# ---------------------------------------------------------------------- #

# Get Pekel monthly composites within all Central Valley HUCs
# MyDrive/WORK/__PLACE/GEE_output/pekel_monthly_CV_HUC8s_Albers.csv

data_url = "1neHmlcTseqKTlF5tRs38Ruawzua-hCtc"
f.csv <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", data_url))
pk <- process_pekel(f.csv, nodata_threshold)

# Somehow date info goes awry
pk$date <- as.Date(pk$date, format = "%Y-%m-%d")
```


```{r, echo=FALSE, message = FALSE, warning = FALSE}

# ---------------------------------------------------------------------- #
# ------ Function - Process DSWE data ------
# ---------------------------------------------------------------------- #

process_dswe <- function(df, nodata_cap) {
  
# X0 = no data, # X1 = not water # X2 = water (high), X3 = water(low conf), X4 = wetland, X5 = water/wetand (low), nodata = clouds, etc., null = no data was present
  colnames(df) <- c("name", 'huc8', "notwater", "water", "water2", "water3", "water4", "nodata", "null", "sqkm")

# Convert name to character
  df <-  df %>% mutate_if(is.factor, as.character) 
  
# First row is useless
  df <- df[-1,]
  
# Split name to extract date
 #  df$name <- substr(df$name, 1, nchar(df$name) - 4)
 #  df.split <- strsplit(df$name, "\\_")
 #  df <- transform(df, year = sapply(df.split, "[[", 2), month = sapply(df.split, "[[", 3))
  
# Split name to extract date 
  df$name <- gsub("[[:punct:]]", "", df$name)
  df$name <- gsub("[a-zA-Z ]", "", df$name)
  df$year <- substr(df$name, 1, 4)
  df$month <- substr(df$name, 5, nchar(df$name))

# Flip the order of the files from starting at the beginning of the year to the end (i.e, file 1 = Dec)
  #df$inverted_month <- plyr::mapvalues(df$month, from = c(seq(1, 12)), to = c(seq(12, 1)))
  #df$month <- df$inverted_month
  #df$inverted_month <- NULL
  
# Convert to date
  df$date <- as.Date(paste(df$year, df$month, "01", sep = "-"), format = "%Y-%m-%d")

# Convert from  m^2 to km^2: 1m^2 = 1e-6 km^2
  # df$notwater <- df$notwater * 0.000001  
  # df$nodata <- df$nodata * 0.000001
  # df$water <- df$water * 0.000001  
  # df$water2 <- df$water2 * 0.000001  
  # df$water3 <- df$water3 * 0.000001  
  # df$water4 <- df$water4 * 0.000001 
  
  
# Convert from  pixels to km^2: 1 pixel = 900 m^2 = 0.0009 km^2
  df$notwater <- df$notwater * 0.0009  
  df$nodata <- df$nodata * 0.0009
  df$water <- df$water * 0.0009 
  df$water2 <- df$water2 * 0.0009  
  df$water3 <- df$water3 * 0.0009 
  df$water4 <- df$water4 * 0.0009 
  df$null <- df$null * 0.0009
  
  
# Convert "water" --> NA in rows where nodata > x% of the total imaged data
# Leaving out 'null' b/c typically that's generated by a missing image
  df$sum <- rowSums(df[, c("nodata", "notwater", "water", "water2", "water3", "water4")], na.rm = T)
  df$pct <- df$nodata/df$sum * 100
  df$water[df$pct > nodata_cap] <- NA
  df$water2[df$pct > nodata_cap] <- NA
  df$water3[df$pct > nodata_cap] <- NA
  df$water4[df$pct > nodata_cap] <- NA
  
# clean up
  df$name <- NULL
  df$year <- NULL
  df$month <- NULL
  df$sqkm <- NULL
  df$null <- NULL

return(df)

}

```


```{r,echo=FALSE, message = FALSE, warning = FALSE}

# ---------------------------------------------------------------------- #
#### Get DSWE unbuffered data ####
# ---------------------------------------------------------------------- #

# Get DSWE monthly composites across entire CV HUCSs
# my drive/WORK/__PLACE/GEE_output/dswe2_0_monthly_HUC_summaries_1984_2016.csv  4 Mar 2019 edition

data_url = "1UPQNdonpw0I23yAIAJm0rnq198maTSBu"
f.csv <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", data_url))
ds <- process_dswe(f.csv, nodata_threshold)

```


```{r,echo=FALSE, message = FALSE, warning = FALSE}

# ---------------------------------------------------------------------- #
#### Merge Pekel and DSWE data ####
# ---------------------------------------------------------------------- #

# Add columns to Pekel data to match DSWE
pk$water2 <- NA
pk$water3 <- NA
pk$water4 <- NA

# Add ID tag to both 
ds$type <-  'dswe'
pk$type <- 'pekel'

# Combine
pk_ds <- rbind(pk, ds)

```


```{r, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}

# ---------------------------------------------------------------------- #
#### Stream gage - read in gage info (not data) for CV HUCs ####
# ---------------------------------------------------------------------- #

# This will allow us to narrow the list of desired gages based on the data available in each

hucs_all <- c('18020002', '18020003','18020004', '18020005', '18020104', '18020111', '18020115',
'18020116', '18020121', '18020122', '18020123', '18020125', '18020126', '18020128', '18020129', '18020151',
'18020152', '18020153', '18020154', '18020155', '18020156', '18020157', '18020158', '18020159', '18020161',
'18020162', '18020163', '18030001', '18030002', '18030003', '18030004', '18030005', '18030006', '18030007',
'18030009', '18030010', '18030012', '18040001', '18040002','18040003', '18040006', '18040007',
'18040008', '18040009', '18040010', '18040011', '18040012', '18040013', '18040014', '18040051')

# Get general info about all gages in Central Valley HUCs from NWIS
gage_info <- function(x) {
  df <- fread(sprintf("https://waterservices.usgs.gov/nwis/site/?format=rdb&huc=%s&seriesCatalogOutput=true&siteStatus=all&hasDataTypeCd=dv,aw", x), check.names = FALSE, header=TRUE)
}

# Consolidate all files
g <- do.call(rbind, lapply(hucs_all, gage_info))

```

####Gage criteria

Even when the gage data criteria are set correctly, the amount of data retrieved might be off. Site 11201800, for instance, has a start date of of 1908-10-01 and an end date of 1995-09-04.  That qualifies under the criteria of at least 5 years of data after 1984.  But looking at the data reveals that there are only daily values recorded from 1908-1912, and then a few from 1994 onward.

Site 11451720 supposedly has data from 1958 to present, but in fact it stops in 1980 before briefly picking up again for two weeks at the end of 1997. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}

# ---------------------------------------------------------------------- #
#### Stream gage - set criteria for gage selection ####
# ---------------------------------------------------------------------- #

# Format dates of data retrieval
g$begin_date <- as.Date(g$begin_date, format = "%Y-%m-%d")
g$end_date <- as.Date(g$end_date, format = "%Y-%m-%d")

# Calculate active time for each gage
g$time_active <- g$end_date - g$begin_date

# Only get gages with >= 5 years of data after 1984
# 'Difftime' reported in days but is unreliable as an indication of true data collection period
g.sub <- subset(g, end_date > "1989-01-01" & time_active > 1800 & (parm_cd == '00060' | parm_cd == '00065'))

# Get unique sites
sites <- unique(g.sub$site_no)

# Make sure site names have 8 characters; others aren't recognized in the automatic retrieval URL
sites <- subset(sites, nchar(sites) == 8)

# Make file with unique sites and corresponding HUC #
site_info <- as.data.frame(g[, c('site_no', 'huc_cd')])
site_info <- site_info[-1,]
site_info <- unique(site_info[c("site_no", "huc_cd")])

```



```{r, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}

# ---------------------------------------------------------------------- #
#### Stream gage - get data from selected sites ####
# ---------------------------------------------------------------------- #

# Set up holder for all site info  
sg <- data.frame()

# Get monthly info from identified stream gages
for (site in sites) {

  file.next <- fread(sprintf("https://waterservices.usgs.gov/nwis/stat/?format=rdb&sites=%s&statReportType=monthly&statTypeCd=all&missingData=on", site), check.names = FALSE, header=TRUE)

  # Ignore no-data files, which start with #
  if (file.next[1, 1] != "#") {
      sg <- rbind(sg, file.next)
  }
}  

# Save a file
 #write.csv(sg, file = "E:/projects/place/data/tables/sg.csv", row.names = F)
# https://drive.google.com/open?id=1GZv8M9dmT9vH8ccetZTHFI_KOXUbhp04

```


```{r, echo=FALSE, warning=FALSE, message=FALSE}

# ---------------------------------------------------------------------- #
#### Stream gage - process aggregated info ####
# ---------------------------------------------------------------------- #

# Data type values are defined here: https://waterservices.usgs.gov/rest/Site-Service.html#outputDataTypeCd
# 00060 = discharge (cu ft/s)
# 00065 = gage height (ft)

# Remove invalid site #s
sg <- sg[sg$site_no %in% sites, ]

# Create date from year and month
sg$date <- as.Date(with(sg, sprintf("%s-%02s-01", year_nu, month_nu)), "%Y-%m-%d")

# Remove dates prior to 1984 and past Pekel data
sg <- subset(sg, year_nu > 1983 & date < max(pk$date, na.rm = TRUE))

# Ensure values are numbers
sg$mean_va <- as.numeric(sg$mean_va)

# Pare down dataset--this requires sg as a data frame
sg <- data.frame(sg)
cols_to_keep <- c("site_no", "parameter_cd", "mean_va", "date")
sg <- sg[, cols_to_keep]

# Restrict to gage height or discharge info only
sg <- subset(sg, parameter_cd == '00065' | parameter_cd == "00060")

# Replace parameter info
sg[sg$parameter_cd == "00060", "parameter_cd"] <- "discharge"
sg[sg$parameter_cd == "00065", "parameter_cd"] <- "gage ht"

# Attribute sites with HUC#
sg <- merge(sg, site_info, by = "site_no")
colnames(sg)[which(colnames(sg) == "huc_cd")] <- "huc8"

# Remove gages in which 
# - all values are the same
# - gage values are negative
# - fewer than 5 years of overlapping data exist.  This is necessary b/c the start, end dates aren't necessarily accurate
sg <- sg %>% 
    group_by(site_no) %>%
    filter(length(unique(mean_va)) > 1) %>%
    filter(mean_va >= 0) %>% 
    filter(n() > 60)

# Assign factors
sg$site_no <- as.factor(sg$site_no)
sg$parameter_cd <- as.factor(sg$parameter_cd)
sg$huc8 <- as.factor(sg$huc8)

```



```{r, echo=FALSE, warning=FALSE, include=FALSE}

# ---------------------------------------------------------------------- #
# Merge imagery and stream gage data across all HUCs####
# ---------------------------------------------------------------------- #

# Merge Pekel/DSWE and gage data by date. Rename sg HUC to avoid confusion between HUCs
pk_ds_sg <-  sg %>% 
   rename(huc8.sg = huc8) %>% 
   right_join(pk_ds, by = c('date')) %>% 
   rename(huc8.img = huc8)

# Assign factors 
pk_ds_sg <- pk_ds_sg %>%
  ungroup(site_no) %>% 
      mutate(huc8.img = factor(huc8.img), 
             type = factor(type), 
             site_no = factor(site_no),  # may need to ungroup this one
             parameter_cd = factor(parameter_cd))

# Delete extraneous columns
pk_ds_sg <- pk_ds_sg %>% 
            select(-c(nodata, notwater, sum, pct))

# Assign tag for whether imagery water data is interpolated (Y) or original (N)
pk_ds_sg$interp <- ifelse(is.na(pk_ds_sg$water), 'Y', 'N')
pk_ds_sg$interp <- as.factor(pk_ds_sg$interp)
```

```{r, echo=FALSE, warning=FALSE, include=FALSE}

# ---------------------------------------------------------------------- #
# save file of merged data ####
# ---------------------------------------------------------------------- #
# Save a file
 #write.csv(sg, file = "E:/projects/place/data/tables/sg.csv", row.names = F)
# https://drive.google.com/open?id=1GZv8M9dmT9vH8ccetZTHFI_KOXUbhp04

```




```{r, echo=FALSE, warning=FALSE, include=FALSE}

# ---------------------------------------------------------------------- #
# Calculate correlations between imagery and gages for all HUCs####
# ---------------------------------------------------------------------- #


# Calculate Pearson correlations based on reduced data set (i.e., NA water values are not yet interpolated)
pk_ds_sg.cor <- pk_ds_sg %>% 
  select(-c(water2, water3, water4)) %>% 
  group_by(huc8.img, site_no, type, parameter_cd, huc8.sg) %>% 
  #ungroup() %>% 
  mutate(cor = cor(water, mean_va, use = "pairwise.complete.obs", method = "pearson"),
            n = sum(!is.na(water))) 

```



```{r, echo=FALSE, message=FALSE, include=FALSE}

# ---------------------------------------------------------------------- #
#### Function: Get coefficients for each correlation ####
# ---------------------------------------------------------------------- #

# Function to apply linear fit
lm_fit <- function(f) {
 # print(head(f))
  fit <- lm(water ~ mean_va, data = f)
  results <- broom::tidy(fit)
  f$b <- results$estimate[1]
  f$m <- results$estimate[2]
#  if (residuals(fit)[1][[1]] > 0 & nrow(f) > 3) {  # necessary to put this in to screen overfit lm's...sometimes
    #   }
  return(f)
}

```


```{r, echo = FALSE, message=FALSE, include=FALSE}
# ---------------------------------------------------------------------- #
#### Calculate correlation coefficients for imagery and gages from all HUCs ####
# ---------------------------------------------------------------------- #

#ignore hucs in which all imagery values are NA
pk_ds_sg.cor$b <- 0
pk_ds_sg.cor$m <- 0
# Start the clock
start_time <- Sys.time()

  pk_ds_sg.int <- pk_ds_sg.cor %>% 
  group_by(huc8.img, site_no, type, parameter_cd, huc8.sg, cor) %>% 
  filter(sum(is.na(water)) != length(water)) %>%
  filter(abs(cor) != 0.0) %>%  # 
  do(lm_fit(.))

end_time <- Sys.time()
end_time - start_time
  
pk_ds_sg.preinterp <- pk_ds_sg.int

```



```{r, echo=FALSE}
# ---------------------------------------------------------------------- #
#### Replace NA imagery values with interpolated data ####
# ---------------------------------------------------------------------- #

pk_ds_sg.int$water <- ifelse(is.na(pk_ds_sg.int$water), pk_ds_sg.int$mean_va * pk_ds_sg.int$m + pk_ds_sg.int$b, pk_ds_sg.int$water)

```


```{r, echo = FALSE}

# ---------------------------------------------------------------------- #
#### Housekeeping ####
# ---------------------------------------------------------------------- #

# HUCs have to be numeric b/c they have different factor levels which kills comparisons
pk_ds_sg.int <- pk_ds_sg.int %>%
  ungroup(huc8) %>% 
      mutate(huc8.img = as.numeric(as.character(huc8.img)),
             huc8.sg = as.numeric(as.character(huc8.sg)),
             type = factor(type))

```


```{r, echo = FALSE, warning = FALSE}

# ---------------------------------------------------------------------- #
#### Get max correlations within HUCs and without ####
# ---------------------------------------------------------------------- #

# Calculate max correlations for imagery with gages in the same HUC
# This returns all 

cor.huc.max <- pk_ds_sg.int %>% 
  filter(huc8.img == huc8.sg) %>% 
  group_by(huc8.img, type) %>% 
  select(-c(date, water, interp, mean_va, b, m)) %>% 
  slice(which.max(cor))

# Calculate max correction given all possible gages
cor.all.max <- pk_ds_sg.int %>% 
  group_by(huc8.img, type) %>% 
  slice(which.max(cor))

# Are the gage and imagery in the same HC?
cor.all.max$same_huc <- 'Y'
cor.all.max$same_huc[cor.all.max$huc8.img != cor.all.max$huc8.sg] <- 'N'

# Get the number of times each gage appears
cor.all.max <- cor.all.max %>% 
  group_by(site_no) %>% 
  mutate(n_site = n())

```


```{r, echo = FALSE, error = FALSE}

# ---------------------------------------------------------------------- #
#### Calculate correlations between gages and all DSWE classes ####
# ---------------------------------------------------------------------- #

# Select only uninterpolated DSWE values
ds_sg <- subset(pk_ds_sg, type == 'dswe' & interp == 'N')

# rowSumming NA and 0 returns NA rather than 0.  This step gets around that and would return 0.
# Sum water plus class 2,3,4
ds_sg$waterplus2 <- rowSums(ds_sg[, c('water', 'water2')], na.rm = TRUE) * 
        ifelse(rowSums(is.na(ds_sg[, c('water', 'water2')])) == ncol(ds_sg[, c('water', 'water2')]), NA, 1) 
ds_sg$waterplus23 <- rowSums(ds_sg[, c('water', 'water2', 'water3')], na.rm = TRUE) * 
        ifelse(rowSums(is.na(ds_sg[, c('water', 'water2', 'water3')])) == ncol(ds_sg[, c('water', 'water2', 'water3')]), NA, 1) 

ds_sg$waterplus24 <- rowSums(ds_sg[, c('water', 'water2', 'water4')], na.rm = TRUE) * 
        ifelse(rowSums(is.na(ds_sg[, c('water', 'water2', 'water4')])) == ncol(ds_sg[, c('water', 'water2', 'water4')]), NA, 1) 


# Calculate correlations
ds_sg.cor <- ds_sg %>% 

  filter(!is.na(mean_va)) %>% 
  group_by(huc8.img, site_no, parameter_cd) %>% 
  summarize(cor = cor(water, mean_va, use = "complete.obs", method = "pearson"),
                   cor2 = cor(waterplus2, mean_va,use = "complete.obs", method = "pearson"),
                   cor3 = cor(waterplus23, mean_va,use = "complete.obs", method = "pearson"),
                   cor4 = cor(waterplus24, mean_va,use = "complete.obs", method = "pearson"),
            n = sum(!is.na(water))) 

```


```{r, echo = FALSE}

# Write file to R 

save.image(file = file.path(path.out, file.out))


```